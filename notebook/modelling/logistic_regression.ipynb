{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Originally Contributed by**: François Pacaud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to solve a logistic regression problem\n",
    "with JuMP. Logistic regression is a well known method in machine learning,\n",
    "useful when we want to classify binary variables with the help of\n",
    "a given set of features. Fitting a logistic\n",
    "regression problem sums up to find the optimal combination of features maximizing\n",
    "the (log)-likelihood onto a training set. In the point of view of optimization,\n",
    "the resulting problem is convex and differentiable. On a modern optimization\n",
    "glance, it is even conic representable.\n",
    "\n",
    "# Formulating the logistic regression problem\n",
    "\n",
    "Suppose we have a set of training data-point $i = 1, \\cdots, n$, where\n",
    "for each $i$ we have a vector of features $x_i \\in \\mathbb{R}^p$ and a\n",
    "categorical observation $y_i \\in \\{-1, 1\\}$.\n",
    "\n",
    "The log-likelihood is given by\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^n \\log(\\dfrac{1}{1 + \\exp(-y_i \\theta^\\top x_i)})\n",
    "$$\n",
    "and finding the optimal parameter $\\theta$ sums up to find the vector\n",
    "$\\theta$ minimizing the logistic loss function:\n",
    "$$\n",
    "\\min_{\\theta}\\; \\sum_{i=1}^n \\log(1 + \\exp(-y_i \\theta^\\top x_i)) .\n",
    "$$\n",
    "Most of the time, instead of solving directly the previous optimization problem, we\n",
    "prefer to add a regularization term:\n",
    "$$\n",
    "\\min_{\\theta}\\; \\sum_{i=1}^n \\log(1 + \\exp(-y_i \\theta^\\top x_i)) + \\lambda \\| \\theta \\|\n",
    "$$\n",
    "with $\\lambda \\in \\mathbb{R}_+$ a penalty and $\\|.\\|$ a norm function. By adding\n",
    "such a regularization term, we avoid overfitting on the training set and usually\n",
    "achieve a greater score in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformulation as a conic optimization problem\n",
    "By introducing auxiliary variables $t_1, \\cdots, t_n$ and $r$,\n",
    "the optimization problem is equivalent to\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{t, r, \\theta} \\;& \\sum_{i=1}^n t_i + \\lambda r \\\\\n",
    "\\text{subject to } & \\quad t_i \\geq \\log(1 + \\exp(- y_i \\theta^\\top x_i)) \\\\\n",
    "                   & \\quad r \\geq \\|\\theta\\|\n",
    "\\end{aligned}\n",
    "$$\n",
    "Now, the trick is to reformulate the constraints $t_i \\geq \\log(1 + \\exp(- y_i \\theta^\\top x_i))$\n",
    "with the help of the *exponential cone*\n",
    "$$\n",
    "K_{exp} = \\{ (x, y, z) \\in \\mathbb{R}^3 : \\; y \\exp(x / y) \\leq z \\} .\n",
    "$$\n",
    "Indeed, by passing to the exponential, we\n",
    "see that for all $i=1, \\cdots, n$, the constraint $t_i \\geq \\log(1 + \\exp(- y_i \\theta^\\top x_i))$\n",
    "is equivalent to\n",
    "$$\n",
    "\\exp(-t_i) + \\exp(u_i - t_i) \\leq 1\n",
    "$$\n",
    "with $u_i = -y_i \\theta^\\top x_i$. Then, by adding two auxiliary variables\n",
    "$z_{i1}$ and $z_{i2}$ such that $z_{i1} \\geq \\exp(u_i-t_i)$ and $z_{i2} \\geq \\exp(-t_i)$, we get\n",
    "the equivalent formulation\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "(u_i -t_i , 1, z_{i1}) & \\in  K_{exp}  \\\\\n",
    "(-t_i , 1, z_{i2}) & \\in  K_{exp}  \\\\\n",
    "z_{i1} + z_{i2} & \\leq  1\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "In this setting, the conic version of the logistic regression problems writes out\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{t, z, r, \\theta}&  \\; \\sum_{i=1}^n t_i + \\lambda r \\\\\n",
    "\\text{subject to } & \\quad  (u_i -t_i , 1, z_{i1})  \\in  K_{exp}  \\\\\n",
    "                   & \\quad  (-t_i , 1, z_{i2})  \\in  K_{exp}  \\\\\n",
    "                   & \\quad  z_{i1} + z_{i2}  \\leq  1 \\\\\n",
    "                   & \\quad u_i = -y_i x_i^\\top \\theta \\\\\n",
    "                   & \\quad r \\geq \\|\\theta\\|\n",
    "\\end{aligned}\n",
    "$$\n",
    "and thus encompasses $3n + p + 1$ variables and $3n + 1$ constraints ($u_i = -y_i \\theta^\\top x_i$\n",
    "is only a temporary constraint used to clarify the notation).\n",
    "Thus, if $n \\gg 1$, we get a large number of variables and constraints which\n",
    "could imped the resolution in the conic solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting logistic regression with a conic solver\n",
    "It is now time to pass to the implementation. We choose ECOS as a conic solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling JuMP [4076af6c-e467-56ae-b986-b466b2749572]\n",
      "└ @ Base loading.jl:1260\n",
      "┌ Info: Precompiling ECOS [e2685f51-7e38-5353-a97d-a921fd2c8199]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    }
   ],
   "source": [
    "using JuMP\n",
    "using Random\n",
    "using ECOS\n",
    "\n",
    "Random.seed!(2713);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing a function to generate a fake dataset, and where\n",
    "we could tune the correlation between the feature variables. The function\n",
    "is a direct transcription of the one used in [this blog post](http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_dataset (generic function with 3 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_dataset(n_samples=100, n_features=10; corr=0.0)\n",
    "    X = randn(n_samples, n_features)\n",
    "    w = randn(n_features)\n",
    "    y = sign.(X * w)\n",
    "    X .+= 0.8 * randn(n_samples, n_features) # add noise\n",
    "    X .+= corr # this makes it correlated by adding a constant term\n",
    "    X = hcat(X, ones(n_samples, 1))\n",
    "    return X, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a `softplus` function to formulate each constraint\n",
    "$t \\geq \\log(1 + \\exp(u))$ with two exponential cones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softplus (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softplus(model, t, u)\n",
    "    z = @variable(model, [1:2], lower_bound=0.0)\n",
    "    @constraint(model, sum(z) <= 1.0)\n",
    "    @constraint(model, vec([u - t, 1, z[1]]) in MOI.ExponentialCone())\n",
    "    @constraint(model, vec([-t, 1, z[2]]) in MOI.ExponentialCone())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$ regularized logistic regression\n",
    "Then, with the help of the `softplus` function, we could write our\n",
    "optimization model. In the $\\ell_2$ regularization case, the constraint\n",
    "$r \\geq \\|\\theta\\|_2$ rewrites as a second order cone constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_logit_model (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function build_logit_model(X, y, λ)\n",
    "    n, p = size(X)\n",
    "    model = Model()\n",
    "    @variable(model, θ[1:p])\n",
    "    @variable(model, t[1:n])\n",
    "    for i in 1:n\n",
    "        u = - (X[i, :]' * θ) * y[i]\n",
    "        softplus(model, t[i], u)\n",
    "    end\n",
    "    # Add ℓ2 regularization\n",
    "    @variable(model, 0.0 <= reg)\n",
    "    @constraint(model, vec([reg; θ]) in MOI.SecondOrderCone(p+1))\n",
    "    # Define objective\n",
    "    @objective(model, Min, sum(t) + λ * reg)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build one dataset with low correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful here, for large n and p ECOS could fail to converge!\n",
    "n, p = 2000, 100\n",
    "X, y = generate_dataset(n, p, corr=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now solve the logistic regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ECOS 2.0.5 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +0.000e+00  -2.992e+03  +2e+04  7e-01  2e+00  1e+00  1e+00    ---    ---    0  0  - |  -  - \n",
      " 1  +1.179e+02  -2.402e+03  +1e+04  5e-01  1e+00  8e-01  8e-01  0.5091  7e-01   1  1  1 |  0  0\n",
      " 2  +5.249e+02  -8.634e+02  +9e+03  2e-01  8e-01  3e-01  5e-01  0.6266  4e-01   1  1  1 |  0  2\n",
      " 3  +5.058e+02  -2.237e+02  +5e+03  1e-01  4e-01  2e-01  3e-01  0.7833  4e-01   1  1  1 |  4  1\n",
      " 4  +5.306e+02  +1.499e+02  +2e+03  1e-01  2e-01  1e-01  1e-01  0.9791  5e-01   1  1  1 |  6  0\n",
      " 5  +6.810e+02  +5.398e+02  +8e+02  4e-02  7e-02  5e-02  4e-02  0.6823  5e-02   1  1  1 |  1  1\n",
      " 6  +7.429e+02  +6.699e+02  +4e+02  2e-02  4e-02  3e-02  2e-02  0.5618  9e-02   1  1  1 |  0  0\n",
      " 7  +7.594e+02  +6.863e+02  +3e+02  2e-02  2e-02  3e-02  2e-02  0.2830  9e-01   2  2  2 |  0  0\n",
      " 8  +8.075e+02  +7.806e+02  +1e+02  8e-03  7e-03  1e-02  7e-03  0.7068  8e-02   2  2  2 |  2  1\n",
      " 9  +8.207e+02  +8.047e+02  +7e+01  5e-03  4e-03  6e-03  4e-03  0.7722  5e-01   2  2  2 |  5  0\n",
      "10  +8.319e+02  +8.246e+02  +3e+01  2e-03  2e-03  3e-03  2e-03  0.6266  1e-01   3  3  3 |  0  2\n",
      "11  +8.376e+02  +8.346e+02  +1e+01  9e-04  8e-04  1e-03  7e-04  0.7833  2e-01   3  3  3 |  3  1\n",
      "12  +8.398e+02  +8.384e+02  +5e+00  4e-04  4e-04  5e-04  3e-04  0.9791  4e-01   3  3  3 |  5  0\n",
      "13  +8.407e+02  +8.400e+02  +3e+00  2e-04  2e-04  3e-04  2e-04  0.6266  2e-01   3  2  2 |  3  2\n",
      "14  +8.413e+02  +8.410e+02  +1e+00  8e-05  7e-05  1e-04  6e-05  0.7833  2e-01   3  2  2 |  3  1\n",
      "15  +8.414e+02  +8.412e+02  +8e-01  6e-05  5e-05  8e-05  4e-05  0.9791  8e-01   3  2  2 | 10  0\n",
      "16  +8.416e+02  +8.415e+02  +3e-01  3e-05  2e-05  3e-05  2e-05  0.6266  5e-02   3  2  2 |  2  2\n",
      "17  +8.416e+02  +8.416e+02  +1e-01  1e-05  9e-06  1e-05  8e-06  0.6266  9e-02   3  2  2 |  2  2\n",
      "18  +8.417e+02  +8.417e+02  +2e-02  1e-06  1e-06  2e-06  9e-07  0.9791  9e-02   3  2  2 |  2  0\n",
      "19  +8.417e+02  +8.417e+02  +4e-03  3e-07  2e-07  4e-07  2e-07  0.7833  1e-02   3  2  2 |  1  1\n",
      "20  +8.417e+02  +8.417e+02  +1e-03  1e-07  1e-07  1e-07  8e-08  0.6266  5e-02   2  1  2 |  2  2\n",
      "21  +8.417e+02  +8.417e+02  +4e-05  3e-09  3e-09  4e-09  2e-09  0.9791  9e-03   2  1  1 |  1  0\n",
      "22  +8.417e+02  +8.417e+02  +4e-05  3e-09  3e-09  4e-09  2e-09  0.1051  7e-01   2  1  1 | 10 10\n",
      "23  +8.417e+02  +8.417e+02  +3e-05  3e-09  2e-09  3e-09  2e-09  0.3208  3e-01   1  1  1 |  5  5\n",
      "24  +8.417e+02  +8.417e+02  +7e-06  6e-10  5e-10  7e-10  4e-10  0.7833  9e-03   2  1  1 |  1  1\n",
      "\n",
      "OPTIMAL (within feastol=5.6e-10, reltol=8.7e-09, abstol=7.3e-06).\n",
      "Runtime: 1.143727 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "λ = 10.0\n",
    "model = build_logit_model(X, y, λ)\n",
    "JuMP.set_optimizer(model, ECOS.Optimizer)\n",
    "JuMP.optimize!(model)\n",
    "\n",
    "θ♯ = JuMP.value.(model[:θ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the speed of convergence is not that impacted by the correlation\n",
    "of the dataset, nor by the penalty $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse logistic regression\n",
    "We now formulate the logistic problem with a $\\ell_1$ regularization term.\n",
    "The $\\ell_1$ regularization ensures sparsity in the optimal\n",
    "solution of the resulting optimization problem. Luckily, the $\\ell_1$ norm\n",
    "is implemented as a set in `MathOptInterface`. Thus, we could easily formulate\n",
    "the sparse logistic regression problem with the help of a `MOI.NormOneCone`\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_sparse_logit_model (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function build_sparse_logit_model(X, y, λ)\n",
    "    n, p = size(X)\n",
    "    model = Model()\n",
    "    @variable(model, θ[1:p])\n",
    "    @variable(model, t[1:n])\n",
    "    for i in 1:n\n",
    "        u = - (X[i, :]' * θ) * y[i]\n",
    "        softplus(model, t[i], u)\n",
    "    end\n",
    "    # Add ℓ1 regularization\n",
    "    @variable(model, 0.0 <= reg)\n",
    "    @constraint(model, vec([reg; θ]) in MOI.NormOneCone(p+1))\n",
    "    # Define objective\n",
    "    @objective(model, Min, sum(t) + λ * reg)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function to count non-null components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count_nonzero (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nonzero(v::Vector; tol=1e-8) = sum(abs.(v) .<= tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the sparse logistic regression problem on the same dataset as\n",
    "before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero components: 13 (out of 100 features)\n",
      "\n",
      "ECOS 2.0.5 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +0.000e+00  -2.992e+03  +2e+04  7e-01  2e+00  1e+00  1e+00    ---    ---    0  0  - |  -  - \n",
      " 1  +6.285e+01  -1.977e+03  +1e+04  4e-01  1e+00  6e-01  6e-01  0.8798  6e-01   1  1  0 |  0  0\n",
      " 2  +2.575e+02  -7.797e+02  +6e+03  2e-01  6e-01  3e-01  3e-01  0.6266  2e-01   1  0  0 |  0  2\n",
      " 3  +4.195e+02  -2.188e+02  +3e+03  2e-01  4e-01  2e-01  2e-01  0.9791  6e-01   1  0  0 |  6  0\n",
      " 4  +5.673e+02  +1.854e+02  +2e+03  1e-01  2e-01  1e-01  1e-01  0.4588  8e-02   1  0  0 |  0  0\n",
      " 5  +7.636e+02  +6.054e+02  +7e+02  5e-02  8e-02  6e-02  4e-02  0.7482  2e-01   1  0  1 |  0  0\n",
      " 6  +8.697e+02  +8.033e+02  +3e+02  2e-02  3e-02  2e-02  2e-02  0.6243  3e-02   1  0  0 |  1  2\n",
      " 7  +9.154e+02  +8.853e+02  +1e+02  9e-03  1e-02  1e-02  6e-03  0.6266  1e-01   1  0  0 |  2  2\n",
      " 8  +9.371e+02  +9.232e+02  +5e+01  4e-03  7e-03  5e-03  3e-03  0.6266  1e-01   1  0  0 |  2  2\n",
      " 9  +9.471e+02  +9.408e+02  +2e+01  2e-03  3e-03  2e-03  1e-03  0.9791  4e-01   1  0  0 |  5  0\n",
      "10  +9.522e+02  +9.494e+02  +1e+01  9e-04  1e-03  1e-03  6e-04  0.6266  1e-01   1  1  1 |  2  2\n",
      "11  +9.542e+02  +9.527e+02  +5e+00  5e-04  7e-04  6e-04  3e-04  0.9791  5e-01   1  1  0 |  6  0\n",
      "12  +9.555e+02  +9.549e+02  +2e+00  2e-04  3e-04  2e-04  1e-04  0.6266  6e-02   1  1  1 |  2  2\n",
      "13  +9.561e+02  +9.559e+02  +8e-01  7e-05  1e-04  9e-05  4e-05  0.7833  2e-01   2  1  1 |  3  1\n",
      "14  +9.564e+02  +9.563e+02  +2e-01  2e-05  3e-05  3e-05  1e-05  0.9791  3e-01   2  1  1 |  4  0\n",
      "15  +9.564e+02  +9.564e+02  +5e-02  5e-06  7e-06  6e-06  3e-06  0.7833  1e-02   1  0  1 |  1  1\n",
      "16  +9.564e+02  +9.564e+02  +2e-02  2e-06  3e-06  2e-06  1e-06  0.6266  5e-02   2  1  1 |  2  2\n",
      "17  +9.565e+02  +9.565e+02  +5e-03  4e-07  7e-07  5e-07  3e-07  0.7833  9e-03   2  1  0 |  1  1\n",
      "18  +9.565e+02  +9.565e+02  +2e-03  2e-07  3e-07  2e-07  1e-07  0.6266  5e-02   2  0  0 |  2  2\n",
      "19  +9.565e+02  +9.565e+02  +4e-04  4e-08  6e-08  5e-08  2e-08  0.7833  9e-03   2  0  0 |  1  1\n",
      "20  +9.565e+02  +9.565e+02  +2e-04  2e-08  2e-08  2e-08  9e-09  0.6266  5e-02   0  0  0 |  2  2\n",
      "21  +9.565e+02  +9.565e+02  +4e-05  3e-09  5e-09  4e-09  2e-09  0.7833  9e-03   1  0  0 |  1  1\n",
      "22  +9.565e+02  +9.565e+02  +2e-05  1e-09  2e-09  2e-09  9e-10  0.6266  5e-02   0  0  0 |  2  2\n",
      "23  +9.565e+02  +9.565e+02  +3e-06  3e-10  5e-10  4e-10  2e-10  0.7833  9e-03   0  0  0 |  1  1\n",
      "\n",
      "OPTIMAL (within feastol=4.9e-10, reltol=3.6e-09, abstol=3.5e-06).\n",
      "Runtime: 0.984438 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "λ = 10.0\n",
    "sparse_model = build_sparse_logit_model(X, y, λ)\n",
    "JuMP.set_optimizer(sparse_model, ECOS.Optimizer)\n",
    "JuMP.optimize!(sparse_model)\n",
    "\n",
    "θ♯ = JuMP.value.(sparse_model[:θ])\n",
    "println(\"Number of non-zero components: \", count_nonzero(θ♯),\n",
    "        \" (out of \", p, \" features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "A direct extension would be to consider the sparse logistic regression with\n",
    "*hard* thresholding, which, on contrary to the *soft* version using a $\\ell_1$ regularization,\n",
    "adds an explicit cardinality constraint in its formulation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\theta} & \\; \\sum_{i=1}^n \\log(1 + \\exp(-y_i \\theta^\\top x_i)) + \\lambda \\| \\theta \\|_2^2 \\\\\n",
    "\\text{subject to } & \\quad \\| \\theta \\|_0 <= k\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $k$ is the maximum number of non-zero components in the vector $\\theta$,\n",
    "and $\\|.\\|_0$ is the $\\ell_0$ pseudo-norm:\n",
    "$$\n",
    "\\| x\\|_0 = \\#\\{i : \\; x_i \\neq 0\\}\n",
    "$$\n",
    "\n",
    "The cardinality constraint $\\|\\theta\\|_0 \\leq k$ could be reformulated with\n",
    "binary variables. Thus the hard sparse regression problem could be solved\n",
    "by any solver supporting mixed integer conic problems.\n",
    "\n",
    "## References\n",
    "1. Logistic regression — MOSEK Fusion API. Available at: https://docs.mosek.com/9.2/pythonfusion/case-studies-logistic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
